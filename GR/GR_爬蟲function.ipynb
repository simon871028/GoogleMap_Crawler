{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "91edd6a104d1025945ba3544a50f130edccffd432672f119b1530b3e465963de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from datetime import datetime\n",
    "import os\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#開啟google\n",
    "browser = webdriver.Chrome('./chromedriver')\n",
    "#設定要前往的網址\n",
    "url = 'https://www.google.com.tw/maps?hl=zh-TW&tab=rl&authuser=0'  \n",
    "#前往該網站\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設立搜尋字元\n",
    "search = '台北 全聯福利中心'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填入網站搜尋內容\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'tactile-searchbox-input')))\n",
    "#網頁元素定位\n",
    "search_input = browser.find_elements_by_class_name('tactile-searchbox-input')[0]\n",
    "#輸入搜尋內容\n",
    "search_input.send_keys(search)\n",
    "#搜尋\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'searchbox-searchbutton-container')))\n",
    "#網頁元素定位\n",
    "search_click = browser.find_elements_by_class_name('searchbox-searchbutton-container')[0]\n",
    "#點下去~\n",
    "search_click.click()"
   ]
  },
  {
   "source": [
    "#名字與今天日期\n",
    "namel=[]\n",
    "while(True):\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    web_out=soup(browser.page_source,\"lxml\")\n",
    "    name=web_out.find_all(\"h3\",class_=\"section-result-title\")\n",
    "    for i in name:\n",
    "        namel.append(i.text)\n",
    "    try:\n",
    "        next_page = browser.find_elements_by_class_name('n7lv7yjyC35__button')[1]\n",
    "        next_page.click()\n",
    "        Next = True\n",
    "        print('Next page!')# 若遇到下一頁則回到原本建立所有店家名字的清單重新跑過\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        Next = False\n",
    "        print('No next page!')\n",
    "        break\n",
    "print(len(namel))\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 92,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Next page!\n",
      "Next page!\n",
      "Next page!\n",
      "Next page!\n",
      "Next page!\n",
      "Next page!\n",
      "Next page!\n",
      "No next page!\n",
      "149\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = ['全聯福利中心Pxmart 大安', '全聯福利中心Pxmart 大安芳和', '全联福利中心Pxmart 大安黎元', '全聯福利中心Pxmart 大安成功', '全聯福利中心Pxmart 大安樂利', '全聯福利中心Pxmart 大安北師', '全聯福利中心Pxmart 大安永康', '全聯福利中心Pxmart 信義嘉興', '全聯福利中心Pxmart 大安泰順', '全聯福利中心Pxmart 信義莊敬', '全聯福利中心Pxmart 大安延吉', '全聯福利中心Pxmart 信義三興', '全聯福利中心Pxmart 中正牯嶺店', '全聯福利中心Pxmart 中山朱崙', '全聯福利中心Pxmart 大安民榮', '全聯福利中心Pxmart 台北東門', '全聯福利中心Pxmart 中正華山', '全聯福利中心Pxmart 信義黎忠', '全聯福利中心Pxmart 信義六合', '全聯福利中心Pxmart 文山景美', '全聯福利中心Pxmart 大安', '全聯福利中心Pxmart 北市建國', '全聯福利中心Pxmart 西門町', '全聯福利中心Pxmart 大安正聲', '全聯福利中心Pxmart 中山中安', '全聯福利中心Pxmart 萬華萬大', '全聯福利中心Pxmart 中正同安', '全聯福利中心Pxmart 大安居安', '全聯福利中心Pxmart 信義松隆', '全聯福利中心Pxmart 中山興安', '全聯福利中心Pxmart 北市和平', '全聯福利中心Pxmart 信義松興', '全聯福利中心 (吉林店)', '全聯福利中心Pxmart 大安敦南', '全聯福利中心Pxmart 台北西藏', '全聯福利中心Pxmart 中山林森']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(search)# 根據搜尋詞建立資料夾"
   ]
  },
  {
   "source": [
    "Next=True# 建立while迴圈\n",
    "\n",
    "count = 0\n",
    "while Next:\n",
    "    name_ls=[]\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    web_out=soup(browser.page_source,\"lxml\")\n",
    "    name=web_out.find_all(\"h3\",class_=\"section-result-title\")\n",
    "    for i in name:\n",
    "        name_ls.append(i.text)\n",
    "    for n in range(len(name_ls)):\n",
    "        if name_ls[n] not in saved:\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'section-result-title'))) # 等待該格load進去後才執行後續\n",
    "            search_click = browser.find_elements_by_class_name('section-result-title')[n] # 從原本頁面點進該店家\n",
    "            search_click.click()\n",
    "            #抓取地址\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'ugiz4pqJLAG__content')))\n",
    "            address_ls=[]\n",
    "            web_in =soup(browser.page_source,\"lxml\")\n",
    "            address = web_in.find_all('button', {'data-tooltip':\"複製地址\"})\n",
    "            address_ls.append(address[0].get('aria-label').strip().strip('地址: '))\n",
    "            #下拉評論並點開所有評論\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CLASS_NAME, \"gm2-button-alt.jqnFjrOWMVU__button-blue\")))\n",
    "            pane= browser.find_elements_by_class_name(\"gm2-button-alt.jqnFjrOWMVU__button-blue\")[0]\n",
    "            action=ActionChains(browser).move_to_element(pane).perform()\n",
    "            pane.click()\n",
    "            time.sleep(2)\n",
    "            # 點開評論選單\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[7]/div[2]/div')))\n",
    "            menu_click = browser.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[7]/div[2]/div')\n",
    "            menu_click.click()\n",
    "            time.sleep(1)\n",
    "            # 選擇評論類型，在此選擇最新選項\n",
    "            category_click = browser.find_elements_by_class_name('action-menu-entry')[1]\n",
    "            category_click.click()\n",
    "            st= 0 # 看你要滑幾次\n",
    "            stop = False\n",
    "            rev_num=[0]\n",
    "            while True:\n",
    "                # 建立滾輪下拉loop\n",
    "                date_ls = []\n",
    "                pane = browser.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "                date = browser.find_elements_by_class_name('section-review-publish-date')\n",
    "                time.sleep(1)\n",
    "                for item in date:\n",
    "                    date_ls.append(item.text)\n",
    "                for it in date_ls:\n",
    "                    if it == '2 年前':\n",
    "                        stop = True\n",
    "                        break# 若遇到2年前字樣則跳出\n",
    "                st = st + 1\n",
    "                if stop == True:\n",
    "                    break\n",
    "                browser.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)\n",
    "                rev = soup(browser.page_source,\"lxml\")\n",
    "                rev_len=rev.find_all(class_ = 'section-review ripple-container GLOBAL__gm2-body-2')\n",
    "                rev_num.append(len(rev_len))\n",
    "                print(len(rev_num))\n",
    "                if rev_num[-1] == rev_num[-2]:\n",
    "                    break# 針對有些新創店家評論不到2年前，在此根據評論長度判斷是否跳出\n",
    "            print(\">>正在爬取評論...下滑次數：\"+str(st))\n",
    "            time.sleep(1)\n",
    "            # 爬取所有評論(此時會包含到2年前，這邊為爬取建到2年前字樣停止滾動當下所有的評論)\n",
    "            re = soup(browser.page_source,\"lxml\")\n",
    "            all_reviews = re.find_all(class_ = 'section-review ripple-container GLOBAL__gm2-body-2')\n",
    "            #抓取評論\n",
    "            all_star_review = []\n",
    "            all_date_review = []\n",
    "            all_text_review = []\n",
    "            for ar in all_reviews:\n",
    "                # 評論星數\n",
    "                all_star_review.append(str(ar.find(class_ = \"section-review-stars\").get('aria-label').strip().strip(\"顆星\")))\n",
    "                # 評論時間\n",
    "                all_date_review.append(ar.find(class_ = \"section-review-publish-date\").text)\n",
    "                # 評論內容\n",
    "                all_text_review.append(ar.find(class_ = \"section-review-text\").text)\n",
    "            #建立dataframe\n",
    "            reviews = pd.DataFrame({\n",
    "                'Store':name_ls[n],\n",
    "                'Today':today,\n",
    "                'Star Reviews':all_star_review,\n",
    "                'Date':all_date_review,\n",
    "                'Text':all_text_review,\n",
    "                \"Address\":address_ls[0]\n",
    "                })\n",
    "            reviews = reviews[reviews['Date'] != '2 年前'] #在此刪去\n",
    "            reviews = reviews[reviews['Date'] != '3 年前'] #有時會1年前跳3年前故保險起見額外增加此行\n",
    "\n",
    "            # 輸出(至原先創立資料夾)\n",
    "            reviews.to_csv(\"./\"+search+\"/\"+name_ls[n] +\"_所有評論.csv\", encoding = 'UTF-8-sig',index=False)\n",
    "            count = count + 1\n",
    "            # 回個別介面\n",
    "            time.sleep(2)\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'mdc-button__icon')))\n",
    "            BacktoStore_btn = browser.find_elements_by_class_name('mdc-button__icon')[0]\n",
    "            BacktoStore_btn.click()\n",
    "            # 返回搜尋結果\n",
    "            time.sleep(2)\n",
    "            WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'section-back-to-list-button.blue-link.noprint')))\n",
    "            back_btn = browser.find_elements_by_class_name('section-back-to-list-button.blue-link.noprint')[0]\n",
    "            back_btn.click()\n",
    "            time.sleep(2)\n",
    "    try:\n",
    "        next_page = browser.find_elements_by_class_name('n7lv7yjyC35__button')[1]\n",
    "        next_page.click()\n",
    "        Next = True\n",
    "        print('Next page!')# 若遇到下一頁則回到原本建立所有店家名字的清單重新跑過\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        Next = False\n",
    "        print('No next page!')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      ">>正在爬取評論...下滑次數：20\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      ">>正在爬取評論...下滑次數：27\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      ">>正在爬取評論...下滑次數：45\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      ">>正在爬取評論...下滑次數：27\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      ">>正在爬取評論...下滑次數：15\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      ">>正在爬取評論...下滑次數：31\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      ">>正在爬取評論...下滑次數：22\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      ">>正在爬取評論...下滑次數：24\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      ">>正在爬取評論...下滑次數：26\n",
      "No next page!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'address_ls' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3f6de2169e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweb_in\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweb_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'button'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'data-tooltip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"複製地址\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maddress_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aria-label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'地址: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maddress_ls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'address_ls' is not defined"
     ]
    }
   ],
   "source": [
    "web_in =soup(browser.page_source,\"lxml\")\n",
    "address = web_in.find_all('button', {'data-tooltip':\"複製地址\"})\n",
    "address_ls.append(address[0].get('aria-label').strip().strip('地址: '))\n",
    "address_ls"
   ]
  }
 ]
}